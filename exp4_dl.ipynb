{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "uadp4kMXLqPS"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#To unzip and read the csv file inside the zip file\n",
        "\n",
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('/content/BBC News Train.csv.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('extracted_data')"
      ],
      "metadata": {
        "id": "CbCJfajdXpRK"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"extracted_data/BBC News Train.csv\", 'r') as csvfile:\n",
        "    print(f\"First line (header) looks like this:\\n\\n{csvfile.readline()}\")\n",
        "    print(f\"The second line (first data point) looks like this:\\n\\n{csvfile.readline()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SYyHpoxX0_4",
        "outputId": "36e66402-4843-41b1-9096-db517a6f0dbc"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First line (header) looks like this:\n",
            "\n",
            "ArticleId,Text,Category\n",
            "\n",
            "The second line (first data point) looks like this:\n",
            "\n",
            "1833,worldcom ex-boss launches defence lawyers defending former worldcom chief bernie ebbers against a battery of fraud charges have called a company whistleblower as their first witness.  cynthia cooper  worldcom s ex-head of internal accounting  alerted directors to irregular accounting practices at the us telecoms giant in 2002. her warnings led to the collapse of the firm following the discovery of an $11bn (£5.7bn) accounting fraud. mr ebbers has pleaded not guilty to charges of fraud and conspiracy.  prosecution lawyers have argued that mr ebbers orchestrated a series of accounting tricks at worldcom  ordering employees to hide expenses and inflate revenues to meet wall street earnings estimates. but ms cooper  who now runs her own consulting business  told a jury in new york on wednesday that external auditors arthur andersen had approved worldcom s accounting in early 2001 and 2002. she said andersen had given a  green light  to the procedures and practices used by worldcom. mr ebber s lawyers have said he was unaware of the fraud  arguing that auditors did not alert him to any problems.  ms cooper also said that during shareholder meetings mr ebbers often passed over technical questions to the company s finance chief  giving only  brief  answers himself. the prosecution s star witness  former worldcom financial chief scott sullivan  has said that mr ebbers ordered accounting adjustments at the firm  telling him to  hit our books . however  ms cooper said mr sullivan had not mentioned  anything uncomfortable  about worldcom s accounting during a 2001 audit committee meeting. mr ebbers could face a jail sentence of 85 years if convicted of all the charges he is facing. worldcom emerged from bankruptcy protection in 2004  and is now known as mci. last week  mci agreed to a buyout by verizon communications in a deal valued at $6.75bn.,business\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the global variables\n",
        "VOCAB_SIZE = 1000\n",
        "EMBEDDING_DIM = 16\n",
        "MAX_LENGTH = 120\n",
        "TRAINING_SPLIT = 0.8"
      ],
      "metadata": {
        "id": "JnYiaMUXX2uw"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"/content/BBC News Train.csv\"\n",
        "data = np.loadtxt(data_dir, delimiter=',', skiprows=0, dtype='str', comments=None)\n",
        "print(f\"Shape of the data: {data.shape}\")\n",
        "print(f\"{data[0]}\\n{data[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VV-6o922X4_W",
        "outputId": "59b8ea6d-e629-4c9a-f2ad-78b023489337"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the data: (1491, 3)\n",
            "['ArticleId' 'Text' 'Category']\n",
            "['1833'\n",
            " 'worldcom ex-boss launches defence lawyers defending former worldcom chief bernie ebbers against a battery of fraud charges have called a company whistleblower as their first witness.  cynthia cooper  worldcom s ex-head of internal accounting  alerted directors to irregular accounting practices at the us telecoms giant in 2002. her warnings led to the collapse of the firm following the discovery of an $11bn (£5.7bn) accounting fraud. mr ebbers has pleaded not guilty to charges of fraud and conspiracy.  prosecution lawyers have argued that mr ebbers orchestrated a series of accounting tricks at worldcom  ordering employees to hide expenses and inflate revenues to meet wall street earnings estimates. but ms cooper  who now runs her own consulting business  told a jury in new york on wednesday that external auditors arthur andersen had approved worldcom s accounting in early 2001 and 2002. she said andersen had given a  green light  to the procedures and practices used by worldcom. mr ebber s lawyers have said he was unaware of the fraud  arguing that auditors did not alert him to any problems.  ms cooper also said that during shareholder meetings mr ebbers often passed over technical questions to the company s finance chief  giving only  brief  answers himself. the prosecution s star witness  former worldcom financial chief scott sullivan  has said that mr ebbers ordered accounting adjustments at the firm  telling him to  hit our books . however  ms cooper said mr sullivan had not mentioned  anything uncomfortable  about worldcom s accounting during a 2001 audit committee meeting. mr ebbers could face a jail sentence of 85 years if convicted of all the charges he is facing. worldcom emerged from bankruptcy protection in 2004  and is now known as mci. last week  mci agreed to a buyout by verizon communications in a deal valued at $6.75bn.'\n",
            " 'business']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first 5 labels from the first column\n",
        "print(f\"The first 5 labels are {data[:5, 0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u58mF_9aYN54",
        "outputId": "52a96705-208e-43bf-a84d-a0f4d520624b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first 5 labels are ['ArticleId' '1833' '154' '1101' '1976']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Reload the CSV file, but skip the first row which contains the headers\n",
        "data = np.loadtxt(\"/content/BBC News Train.csv\", delimiter=',', skiprows=1, dtype='str', comments=None)\n",
        "\n",
        "# Now the dataset will not include the header, and you'll work with the actual data\n",
        "print(f\"There are {len(data)} sentence-label pairs in the dataset.\\n\")\n",
        "\n",
        "# Check the number of words in the first sentence (now it should be correct)\n",
        "print(f\"First sentence has {len(data[0, 1].split())} words.\\n\")\n",
        "\n",
        "# Print the first 5 labels (category column, index 0)\n",
        "print(f\"The first 5 labels are {data[:5, 0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OAd7uHqYoPu",
        "outputId": "d3399f29-86d6-4a28-91b2-2241da40a8d1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1490 sentence-label pairs in the dataset.\n",
            "\n",
            "First sentence has 301 words.\n",
            "\n",
            "The first 5 labels are ['1833' '154' '1101' '1976' '917']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTIONS: train_val_datasets\n",
        "def train_val_datasets(data,train_split=1780/2225):\n",
        "    '''\n",
        "    Splits data into traning and validations sets\n",
        "\n",
        "    Args:\n",
        "        data (np.array): array with two columns, first one is the label, the second is the text\n",
        "\n",
        "    Returns:\n",
        "        (tf.data.Dataset, tf.data.Dataset): tuple containing the train and validation datasets\n",
        "    '''\n",
        "   ### START CODE HERE ###\n",
        "\n",
        "    # Compute the number of samples that will be used for training\n",
        "    train_size = int(len(data) * train_split)\n",
        "\n",
        "    # Slice the dataset to get only the texts and labels\n",
        "    texts = data[:, 1]  # texts are in the second column\n",
        "    labels = data[:, 0]  # labels are in the first column\n",
        "\n",
        "    # Split the texts and labels into train/validation splits\n",
        "    train_texts = texts[:train_size]\n",
        "    validation_texts = texts[train_size:]\n",
        "    train_labels = labels[:train_size]\n",
        "    validation_labels = labels[train_size:]\n",
        "\n",
        "    # Create the train and validation datasets from the splits\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((train_texts, train_labels))\n",
        "    validation_dataset = tf.data.Dataset.from_tensor_slices((validation_texts, validation_labels))\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "\n",
        "    return train_dataset, validation_dataset\n",
        "\n",
        "# Create the datasets\n",
        "train_dataset, validation_dataset = train_val_datasets(data)\n",
        "print('Name: shalini v       Register Number: 212222240096  ')\n",
        "print(f\"There are {train_dataset.cardinality()} sentence-label pairs for training.\\n\")\n",
        "print(f\"There are {validation_dataset.cardinality()} sentence-label pairs for validation.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxvEjtLDYt0l",
        "outputId": "01654f98-e88a-40cd-f784-d1b45c59fa50"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: shalini v       Register Number: 212222240096  \n",
            "There are 1192 sentence-label pairs for training.\n",
            "\n",
            "There are 298 sentence-label pairs for validation.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize_func(sentence):\n",
        "    \"\"\"\n",
        "    Removes a list of stopwords\n",
        "\n",
        "    Args:\n",
        "        sentence (tf.string): sentence to remove the stopwords from\n",
        "\n",
        "    Returns:\n",
        "        sentence (tf.string): lowercase sentence without the stopwords\n",
        "    \"\"\"\n",
        "    # List of stopwords\n",
        "    stopwords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"her\", \"here\",  \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\",  \"i\", \"if\", \"in\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\",  \"should\", \"so\", \"some\", \"such\", \"than\", \"that\",  \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"these\", \"they\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\",  \"were\", \"what\",  \"when\", \"where\", \"which\", \"while\", \"who\", \"whom\", \"why\", \"why\", \"with\", \"would\", \"you\",  \"your\", \"yours\", \"yourself\", \"yourselves\", \"'m\",  \"'d\", \"'ll\", \"'re\", \"'ve\", \"'s\", \"'d\"]\n",
        "\n",
        "    # Sentence converted to lowercase-only\n",
        "    sentence = tf.strings.lower(sentence)\n",
        "\n",
        "    # Remove stopwords\n",
        "    for word in stopwords:\n",
        "        if word[0] == \"'\":\n",
        "            sentence = tf.strings.regex_replace(sentence, rf\"{word}\\b\", \"\")\n",
        "        else:\n",
        "            sentence = tf.strings.regex_replace(sentence, rf\"\\b{word}\\b\", \"\")\n",
        "\n",
        "    # Remove punctuation\n",
        "    sentence = tf.strings.regex_replace(sentence, r'[!\"#$%&()\\*\\+,-\\./:;<=>?@\\[\\\\\\]^_`{|}~\\']', \"\")\n",
        "\n",
        "\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "ejiJTzaLY4Xb"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: fit_vectorizer\n",
        "def fit_vectorizer(train_sentences, standardize_func):\n",
        "    '''\n",
        "    Defines and adapts the text vectorizer\n",
        "\n",
        "    Args:\n",
        "        train_sentences (tf.data.Dataset): sentences from the train dataset to fit the TextVectorization layer\n",
        "        standardize_func (FunctionType): function to remove stopwords and punctuation, and lowercase texts.\n",
        "    Returns:\n",
        "        TextVectorization: adapted instance of TextVectorization layer\n",
        "    '''\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    # If train_sentences is a NumPy array, convert it to a TensorFlow Dataset\n",
        "    if isinstance(train_sentences, np.ndarray):\n",
        "        train_sentences = tf.data.Dataset.from_tensor_slices(train_sentences)\n",
        "\n",
        "    # Initialize the TextVectorization layer\n",
        "    vectorizer = tf.keras.layers.TextVectorization(\n",
        "        standardize=standardize_func,     # Using the custom standardization function\n",
        "        max_tokens=1000,                  # Set the vocabulary size to 1000\n",
        "        output_sequence_length=120        # Set the maximum sequence length to 120\n",
        "    )\n",
        "\n",
        "    # Adapt the vectorizer to the training sentences\n",
        "    vectorizer.adapt(train_sentences.map(lambda text: text))  # Adapting only the text from (text, label) pairs\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return vectorizer"
      ],
      "metadata": {
        "id": "F86bcphGZE-h"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create the vectorizer\n",
        "text_only_dataset = train_dataset.map(lambda text, label: text)\n",
        "vectorizer = fit_vectorizer(text_only_dataset, standardize_func)\n",
        "vocab_size = vectorizer.vocabulary_size()\n",
        "print('Name: shalini v    Register Number: 212222240096    ')\n",
        "print(f\"Vocabulary contains {vocab_size} words\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a6fiKsSZGx3",
        "outputId": "8b764a9c-1138-45b6-95b2-99fd66c7eaf8"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: shalini v    Register Number: 212222240096    \n",
            "Vocabulary contains 1000 words\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels_only = train_dataset.map(lambda text, label: label)\n",
        "validation_labels_only = validation_dataset.map(lambda text, label: label)\n",
        "\n",
        "label_encoder = fit_label_encoder(train_labels_only,validation_labels_only)\n",
        "print('Name: shalini v      Register Number: 212222240096     ')\n",
        "print(f'Unique labels: {label_encoder.get_vocabulary()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4arXbxr9ZU-d",
        "outputId": "a2e01de9-b1c7-4d99-9316-7f4fe6752c48"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: shalini v      Register Number: 212222240096     \n",
            "Unique labels: ['998', '997', '995', '993', '992', '991', '990', '989', '988', '987', '986', '985', '984', '983', '982', '980', '98', '979', '978', '974', '973', '972', '970', '969', '967', '964', '962', '960', '96', '959', '958', '957', '956', '955', '953', '951', '950', '949', '948', '947', '946', '944', '943', '941', '940', '94', '937', '936', '935', '934', '932', '930', '93', '929', '926', '924', '923', '920', '92', '919', '918', '917', '916', '912', '91', '909', '907', '906', '905', '904', '903', '902', '901', '900', '90', '899', '898', '897', '896', '894', '893', '892', '891', '890', '889', '888', '886', '885', '883', '882', '881', '880', '88', '879', '878', '877', '876', '873', '871', '870', '867', '865', '864', '863', '860', '86', '859', '858', '857', '856', '855', '853', '850', '85', '848', '844', '843', '842', '840', '839', '836', '835', '834', '833', '832', '831', '83', '829', '828', '826', '825', '824', '822', '820', '82', '818', '817', '816', '815', '814', '813', '812', '811', '809', '805', '804', '803', '802', '801', '800', '80', '797', '794', '793', '792', '791', '790', '79', '789', '788', '787', '785', '783', '780', '78', '778', '777', '775', '774', '773', '772', '771', '770', '767', '766', '764', '762', '761', '760', '76', '759', '758', '757', '756', '755', '754', '753', '750', '75', '749', '748', '747', '744', '743', '741', '740', '74', '739', '738', '736', '735', '734', '733', '731', '730', '729', '728', '727', '726', '722', '720', '718', '716', '714', '712', '711', '710', '709', '707', '706', '705', '702', '701', '70', '7', '699', '697', '696', '695', '693', '690', '69', '689', '688', '687', '686', '685', '684', '682', '68', '679', '678', '677', '675', '674', '673', '672', '670', '666', '665', '663', '661', '660', '66', '658', '657', '656', '655', '654', '651', '65', '649', '648', '647', '645', '644', '643', '641', '639', '637', '636', '634', '632', '631', '630', '63', '629', '628', '627', '623', '622', '62', '619', '618', '617', '616', '615', '614', '613', '612', '611', '610', '61', '609', '607', '605', '603', '602', '601', '597', '595', '593', '590', '589', '588', '587', '583', '581', '578', '577', '575', '574', '572', '571', '570', '569', '566', '565', '563', '561', '560', '559', '557', '555', '552', '550', '549', '548', '547', '544', '543', '542', '54', '538', '537', '536', '534', '533', '530', '53', '528', '527', '526', '522', '520', '518', '517', '514', '513', '512', '510', '508', '507', '506', '500', '50', '497', '496', '495', '494', '492', '491', '490', '49', '488', '487', '486', '485', '484', '482', '480', '479', '478', '477', '476', '475', '474', '473', '470', '47', '467', '466', '465', '464', '461', '460', '46', '458', '456', '455', '454', '453', '450', '449', '447', '445', '444', '443', '441', '440', '44', '439', '437', '436', '435', '434', '433', '431', '430', '429', '428', '427', '426', '425', '424', '423', '420', '42', '419', '418', '416', '415', '413', '410', '408', '407', '406', '405', '403', '401', '400', '40', '4', '399', '396', '395', '393', '392', '391', '390', '39', '389', '388', '387', '386', '384', '380', '38', '378', '377', '375', '374', '372', '371', '370', '37', '366', '365', '364', '363', '362', '361', '360', '359', '358', '356', '355', '352', '351', '35', '346', '345', '344', '343', '342', '341', '340', '339', '335', '334', '331', '33', '329', '328', '327', '325', '324', '323', '320', '32', '319', '318', '317', '316', '315', '314', '312', '311', '310', '31', '307', '306', '305', '304', '303', '302', '301', '300', '30', '299', '298', '295', '293', '292', '291', '290', '29', '289', '288', '286', '284', '283', '281', '280', '28', '279', '277', '275', '274', '273', '271', '268', '267', '266', '264', '261', '26', '257', '256', '255', '254', '253', '252', '251', '250', '25', '249', '248', '247', '246', '245', '241', '240', '239', '238', '236', '234', '233', '232', '231', '230', '229', '228', '227', '226', '225', '224', '2224', '2223', '2221', '2220', '2219', '2217', '2216', '2214', '2213', '2212', '2210', '221', '2209', '2208', '2207', '2206', '2205', '2204', '2201', '220', '2199', '2198', '2197', '2195', '2193', '2192', '219', '2188', '2187', '2186', '2185', '2184', '2183', '2180', '218', '2179', '2178', '2177', '2175', '2173', '2171', '2170', '217', '2169', '2168', '2166', '2164', '2161', '2160', '216', '2159', '2158', '2157', '2156', '2153', '2152', '2151', '2150', '2149', '2148', '2147', '2145', '2144', '2143', '2142', '2141', '214', '2135', '2133', '2131', '2130', '213', '2129', '2128', '2127', '2121', '2120', '212', '2119', '2118', '2117', '2116', '2115', '2114', '2113', '2112', '2111', '2110', '2109', '2108', '2107', '2102', '2100', '210', '21', '2098', '2096', '2095', '2093', '2091', '209', '2089', '2088', '2086', '2084', '2083', '2082', '2081', '2080', '208', '2079', '2078', '2077', '2076', '2075', '2074', '2073', '2072', '2070', '207', '2069', '2067', '2064', '2063', '2062', '2061', '2060', '2059', '2058', '2056', '2054', '2052', '2050', '2048', '2047', '2046', '2045', '2044', '2043', '2042', '2041', '204', '2036', '2035', '2034', '2032', '2031', '2030', '203', '2029', '2027', '2026', '2023', '2021', '2020', '202', '2019', '2017', '2016', '2015', '2014', '2012', '2010', '201', '2009', '2006', '2004', '2003', '2002', '2001', '2000', '200', '20', '2', '1999', '1998', '1995', '1994', '1993', '1992', '1990', '199', '1989', '1988', '1985', '1984', '1982', '1981', '1980', '198', '1978', '1977', '1976', '1974', '1971', '197', '1969', '1968', '1967', '1966', '1965', '1964', '1963', '1962', '1960', '196', '1955', '1952', '1951', '1950', '195', '1949', '1948', '1947', '1946', '1945', '1943', '1942', '1941', '1940', '194', '1939', '1938', '1937', '1935', '1934', '1932', '1931', '1930', '193', '1929', '1928', '1926', '1925', '1921', '1920', '192', '1919', '1917', '1916', '1914', '1913', '1912', '1911', '1909', '1908', '1906', '1904', '1903', '1902', '1901', '1900', '190', '19', '1897', '1896', '1894', '1893', '1891', '189', '1887', '1886', '1885', '1884', '1882', '1881', '1880', '188', '1879', '1878', '1877', '1876', '1875', '1874', '1873', '1872', '1871', '1870', '187', '1868', '1867', '1866', '1864', '1863', '1862', '1860', '186', '1858', '1855', '1854', '1853', '1852', '1851', '1850', '1848', '1847', '1846', '1844', '1843', '1841', '1840', '184', '1838', '1836', '1835', '1834', '1833', '1831', '1830', '183', '1827', '1825', '1824', '1821', '1820', '182', '1819', '1818', '1817', '1815', '1814', '1813', '1812', '1811', '181', '1809', '1808', '1806', '1805', '1804', '1803', '1802', '1801', '180', '18', '1799', '1797', '1796', '1795', '1794', '1793', '1792', '1791', '1790', '1788', '1786', '1785', '1784', '1783', '1781', '1778', '1774', '1772', '1771', '177', '1769', '1767', '1766', '1765', '1764', '1763', '1761', '1760', '176', '1759', '1758', '1756', '1755', '1754', '1753', '1752', '1751', '1750', '175', '1749', '1745', '1744', '1742', '1741', '1740', '174', '1739', '1738', '1737', '1736', '1734', '1733', '1731', '1730', '173', '1728', '1726', '1725', '1723', '1718', '1717', '1716', '1715', '1714', '1713', '1711', '1710', '171', '1709', '1708', '1707', '1706', '1702', '1701', '170', '1697', '1696', '1692', '1690', '169', '1689', '1688', '1687', '1686', '1683', '1682', '1681', '1680', '1678', '1677', '1676', '1674', '1670', '167', '1667', '1665', '1664', '1661', '166', '1658', '1657', '1655', '1653', '1652', '1651', '1650', '165', '1649', '1648', '1647', '1646', '1645', '1642', '1641', '1640', '164', '1639', '1638', '1637', '1633', '1632', '1630', '1625', '1624', '1623', '1621', '1620', '1618', '1617', '1615', '1614', '1613', '1612', '1611', '1610', '1609', '1607', '1606', '1605', '1604', '1603', '1602', '1600', '160', '16', '1599', '1598', '1597', '1596', '1595', '1590', '159', '1589', '1588', '1587', '1585', '1584', '1583', '1582', '158', '1579', '1578', '1574', '1572', '1571', '1570', '1569', '1566', '1565', '1561', '1560', '156', '1559', '1557', '1555', '1554', '1553', '1552', '1550', '155', '1548', '1547', '1546', '1544', '1542', '1540', '154', '1538', '1537', '1536', '1534', '1533', '1532', '153', '1529', '1528', '1527', '1526', '1525', '1524', '1523', '1522', '1518', '1517', '1516', '1515', '1511', '1510', '151', '1508', '1506', '1505', '1504', '1503', '1500', '150', '15', '1499', '1498', '1497', '1495', '1494', '1493', '1492', '1491', '1490', '1489', '1488', '1487', '1486', '1485', '1483', '1482', '1481', '1480', '148', '1478', '1475', '1474', '1473', '147', '1469', '1468', '1467', '1466', '1465', '1464', '1462', '1458', '1455', '1454', '1453', '1452', '1451', '1450', '145', '1449', '1446', '1445', '1444', '1443', '1441', '1440', '1439', '1437', '1436', '1435', '1434', '1433', '1432', '1431', '143', '1429', '1428', '1427', '1425', '1424', '1421', '1418', '1415', '1414', '1413', '1412', '1411', '1410', '1409', '1408', '1407', '1406', '1403', '1402', '1401', '1400', '140', '14', '1398', '1397', '1396', '1395', '1394', '1392', '1391', '1390', '1388', '1387', '1386', '1383', '1382', '1381', '1380', '1379', '1378', '1377', '1376', '1375', '1374', '1372', '1370', '137', '1369', '1368', '1366', '1364', '1363', '1362', '1361', '1360', '136', '1358', '1355', '1354', '1351', '1350', '135', '1349', '1348', '1347', '1346', '1344', '1343', '1342', '1338', '1337', '1336', '1333', '1332', '1331', '1330', '1328', '1327', '1326', '1323', '1322', '1321', '1320', '132', '1318', '1314', '1312', '1309', '1308', '1303', '1301', '130', '1299', '1297', '1294', '1292', '1289', '1287', '1283', '1282', '1280', '128', '1279', '1277', '1276', '1275', '1273', '1271', '1269', '1268', '1266', '1264', '1263', '1261', '1260', '126', '1259', '1257', '1254', '1253', '1251', '1250', '125', '1249', '1248', '1247', '1245', '1244', '1243', '1241', '1240', '1239', '1238', '1236', '1235', '1234', '1231', '123', '1229', '1228', '1226', '1223', '1222', '1220', '122', '1218', '1217', '1215', '1214', '1213', '1212', '1211', '1210', '121', '1209', '1208', '1205', '1204', '1203', '1202', '1201', '120', '1199', '1198', '1197', '1195', '1194', '1192', '1190', '119', '1186', '1185', '1184', '1183', '1182', '1179', '1178', '1177', '1175', '1174', '1170', '117', '1169', '1167', '1166', '1164', '1163', '1160', '1159', '1157', '1156', '1155', '1153', '1152', '1150', '115', '1149', '1148', '1147', '1146', '1144', '1143', '1141', '114', '1139', '1137', '1135', '1133', '1130', '1129', '1128', '1127', '1126', '1125', '1124', '1122', '1121', '1120', '1119', '1118', '1117', '1115', '1114', '1113', '1112', '1111', '1110', '111', '1109', '1108', '1106', '1104', '1103', '1102', '1101', '1100', '110', '11', '1099', '1098', '1097', '1096', '1095', '1093', '1091', '1090', '1089', '1088', '1087', '1082', '1081', '1080', '108', '1079', '1078', '1077', '1076', '1075', '1073', '1072', '1071', '1070', '1069', '1066', '1065', '1064', '1062', '1060', '106', '1058', '1057', '1056', '1055', '1054', '1053', '1052', '1050', '105', '1049', '1047', '1045', '1043', '1042', '1041', '1040', '104', '1039', '1038', '1037', '1036', '1034', '1033', '1031', '1030', '1029', '1028', '1027', '1026', '1024', '1023', '1022', '1021', '102', '1017', '1014', '1013', '1012', '1011', '101', '1009', '1008', '1007', '1005', '1004', '1003', '10']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: preprocess_dataset\n",
        "def preprocess_dataset(dataset, text_vectorizer, label_encoder):\n",
        "    \"\"\"Apply the preprocessing to a dataset\n",
        "\n",
        "    Args:\n",
        "        dataset (tf.data.Dataset): dataset to preprocess\n",
        "        text_vectorizer (tf.keras.layers.TextVectorization ): text vectorizer\n",
        "        label_encoder (tf.keras.layers.StringLookup): label encoder\n",
        "\n",
        "    Returns:\n",
        "        tf.data.Dataset: transformed dataset\n",
        "    \"\"\"\n",
        "\n",
        "      ### START CODE HERE ###\n",
        "\n",
        "    # Apply text vectorization and label encoding\n",
        "    dataset = dataset.map(lambda text, label: (text_vectorizer(text), label_encoder(label)))\n",
        "\n",
        "    # Set the batch size to 32\n",
        "    dataset = dataset.batch(32)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# Preprocess your dataset\n",
        "train_proc_dataset = preprocess_dataset(train_dataset, vectorizer, label_encoder)\n",
        "validation_proc_dataset = preprocess_dataset(validation_dataset, vectorizer, label_encoder)\n",
        "\n",
        "\n",
        "train_batch = next(train_proc_dataset.as_numpy_iterator())\n",
        "validation_batch = next(validation_proc_dataset.as_numpy_iterator())\n",
        "print('Name: shalini v      Register Number: 212222240096    ')\n",
        "print(f\"Shape of the train batch: {train_batch[0].shape}\")\n",
        "print(f\"Shape of the validation batch: {validation_batch[0].shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgrB3_PpZW0W",
        "outputId": "70d03b56-9b4a-41de-d633-e2a922a33441"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: shalini v      Register Number: 212222240096    \n",
            "Shape of the train batch: (32, 120)\n",
            "Shape of the validation batch: (32, 120)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: create_model\n",
        "def create_model():\n",
        "    \"\"\"\n",
        "    Creates a text classifier model\n",
        "    Returns:\n",
        "      tf.keras Model: the text classifier model\n",
        "    \"\"\"\n",
        "\n",
        "      ### START CODE HERE ###\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.Input(shape=(120,)),  # Input layer with a fixed sequence length of 120\n",
        "        tf.keras.layers.Embedding(input_dim=1000, output_dim=16),  # Smaller embedding layer (reduce output_dim)\n",
        "        tf.keras.layers.GlobalAveragePooling1D(),  # Global average pooling to reduce complexity\n",
        "        tf.keras.layers.Dense(16, activation='relu'),  # Reduced Dense layer size to 16 units\n",
        "        tf.keras.layers.Dense(5, activation='softmax')  # Output layer with 5 units for 5 classes\n",
        "    ])\n",
        "\n",
        "    # Compile the model with appropriate loss, optimizer, and metrics\n",
        "    model.compile(\n",
        "        loss='sparse_categorical_crossentropy',  # Use sparse categorical cross-entropy for integer-encoded labels\n",
        "        optimizer='adam',  # Adam optimizer\n",
        "        metrics=['accuracy']  # Track accuracy\n",
        "    )\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return model\n",
        "\n",
        "# Get the untrained model\n",
        "model = create_model()"
      ],
      "metadata": {
        "id": "-f8T0V-OZaFv"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch = train_proc_dataset.take(1)\n",
        "\n",
        "try:\n",
        "\tmodel.evaluate(example_batch, verbose=False)\n",
        "except:\n",
        "\tprint(\"Your model is not compatible with the dataset you defined earlier. Check that the loss function and last layer are compatible with one another.\")\n",
        "else:\n",
        "\tpredictions = model.predict(example_batch, verbose=False)\n",
        "\tprint(f\"predictions have shape: {predictions.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9Wr_oQwZ51_",
        "outputId": "7c115294-08a6-4059-c850-c7c4077bcbef"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your model is not compatible with the dataset you defined earlier. Check that the loss function and last layer are compatible with one another.\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def fit_label_encoder(train_labels, validation_labels):\n",
        "    \"\"\"Fits a label encoder to the unique labels in the train and validation datasets.\n",
        "\n",
        "    Args:\n",
        "        train_labels (tf.data.Dataset): Training labels dataset.\n",
        "        validation_labels (tf.data.Dataset): Validation labels dataset.\n",
        "\n",
        "    Returns:\n",
        "        tf.keras.layers.StringLookup: Fitted label encoder.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get unique labels from both train and validation datasets\n",
        "    unique_train_labels = set()\n",
        "    for label in train_labels.as_numpy_iterator():\n",
        "        unique_train_labels.add(label.item())  # Convert numpy scalar to Python scalar\n",
        "\n",
        "    unique_validation_labels = set()\n",
        "    for label in validation_labels.as_numpy_iterator():\n",
        "        unique_validation_labels.add(label.item())  # Convert numpy scalar to Python scalar\n",
        "\n",
        "    # Combine unique labels from both datasets\n",
        "    unique_labels = sorted(list(unique_train_labels | unique_validation_labels))\n",
        "\n",
        "    # Create and fit the label encoder\n",
        "    label_encoder = tf.keras.layers.StringLookup(vocabulary=unique_labels, oov_token=None)\n",
        "\n",
        "    return label_encoder"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "edskzNuUaaWV"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graphs(history, metric):\n",
        "    plt.plot(history.history[metric])\n",
        "    plt.plot(history.history[f'val_{metric}'])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(metric)\n",
        "    plt.legend([metric, f'val_{metric}'])\n",
        "    plt.show()\n",
        "print('Name: shalini      Register Number: 212222240096     ')\n",
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "6_71gb03aDjH",
        "outputId": "d476553b-d6de-4922-f79d-be768864138d"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: shalini      Register Number: 212222240096     \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'history' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-83ebc75fbd3b>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Name: shalini      Register Number: 212222240096     '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mplot_graphs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mplot_graphs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M1t6SokyaFVz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}